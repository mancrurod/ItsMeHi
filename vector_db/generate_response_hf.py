# vector_db/generate_response_hf.py

import os
from huggingface_hub import InferenceClient
from dotenv import load_dotenv
import streamlit as st
import traceback

# === Load environment variables ===
load_dotenv()

# === Connect to Hugging Face Inference API ===
api_token = os.getenv("HF_API_TOKEN")
model_name = os.getenv("HF_GENERATION_MODEL")

client = InferenceClient(token=api_token)

def generar_respuesta_hf(prompt: str) -> str:
    """Generate a response using Hugging Face Inference API with detailed debugging."""
    try:
        print(f"üöÄ Enviando prompt a HuggingFace: {prompt[:100]}...")
        response = client.text_generation(
            prompt,
            model=model_name,
            max_new_tokens=256,
            temperature=0.7,
            stop_sequences=["###", "</s>"],
            timeout=60,  # ‚è±Ô∏è a√±adir timeout expl√≠cito (por si acaso)
        )
        print(f"‚úÖ Respuesta recibida de HuggingFace.")
        return response.strip()
    except Exception as e:
        error_trace = traceback.format_exc()
        print(f"‚ö° Error real generado:\n{error_trace}")
        st.error(f"‚ö° Error interno: {str(e)}")  # Mostrar error resumido en pantalla
        return "‚ö° El modelo no respondi√≥ a tiempo. Por favor, int√©ntalo de nuevo en unos minutos."
