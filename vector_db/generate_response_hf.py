# vector_db/generate_response_hf.py

import os
from huggingface_hub import InferenceClient
from dotenv import load_dotenv
import streamlit as st
import traceback

# === Load environment variables ===
load_dotenv()

# === Connect to Hugging Face Inference API ===
api_token = os.getenv("HF_API_TOKEN")
model_name = os.getenv("HF_GENERATION_MODEL")

client = InferenceClient(token=api_token)

def generar_respuesta_hf(prompt: str) -> str:
    """Generate a response using Hugging Face Inference API with timeout control."""
    try:
        print(f"üöÄ Enviando prompt a HuggingFace...")
        # Hacemos llamada limitada a m√°ximo 30 segundos
        response = client.text_generation(
            prompt,
            model=model_name,
            max_new_tokens=256,
            temperature=0.7,
            stop_sequences=["###", "</s>"],
            timeout=30,  # ‚è±Ô∏è Forzar timeout de cliente
        )
        print(f"‚úÖ Respuesta recibida de HuggingFace.")
        return response.strip()
    except Exception as e:
        error_trace = traceback.format_exc()
        print(f"‚ö° Error real capturado:\n{error_trace}")
        st.error(f"‚ö° Error interno de generaci√≥n: {str(e)}")
        return "‚ö° El modelo no respondi√≥ o tard√≥ demasiado. Por favor, intenta de nuevo m√°s tarde."
